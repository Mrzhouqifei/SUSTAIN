{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mxnet import gpu\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from gluonts.mx.distribution.neg_binomial import NegativeBinomialOutput\n",
    "from gluonts.mx.trainer import Trainer\n",
    "\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    SetFieldIfNotPresent,\n",
    ")\n",
    "\n",
    "# predictor_deserialized = Predictor.deserialize(Path(\"/tmp/\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = pd.read_csv('raw_data/COVID/time_series_covid19_confirmed_global.csv')\n",
    "indicators = pd.read_excel('raw_data/SUSTAIN database_08Jan2021_Asia and Latin America.xlsx')\n",
    "policy = pd.read_excel('raw_data/SUSTAIN database_09Jan2021_policies_Asia and Latin America.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirmed.head(10)\n",
    "# len(list(set(policy.entity)))\n",
    "# len(list(set(indicators.Country)))\n",
    "# len(list(set(confirmed['Country/Region'])))\n",
    "countries = list(set(policy.entity).intersection(set(confirmed['Country/Region'])).intersection(set(indicators['Country'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dataset(num_series, num_steps):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # size = (num_series, num_steps)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_series = confirmed.groupby('Country/Region').sum().drop(['Lat', 'Long'],axis=1).loc[countries, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = StandardScaler().fit_transform(confirmed_series).T\n",
    "# target = confirmed_series.values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_d = np.zeros(target.shape)#np.concatenate((target, target), axis=0)\n",
    "covariate_d.shape\n",
    "covariate_d = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 340)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_s = [[0] for _ in range(target.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the dataset\n",
    "custom_ds_metadata = {'num_series': len(countries),\n",
    "                      'num_steps': len(confirmed_series),\n",
    "                      'prediction_length': 10,\n",
    "                      'context_length': 10,\n",
    "                      'freq': '1D',\n",
    "                      'start': [pd.Timestamp(confirmed_series.index[0], freq='1D')\n",
    "                                for _ in range(len(confirmed_series))]\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ListDataset([{FieldName.TARGET: target,\n",
    "                         FieldName.START: start,\n",
    "#                          FieldName.FEAT_STATIC_REAL: fsr,\n",
    "                         FieldName.FEAT_DYNAMIC_REAL: [fdr],\n",
    "#                          FieldName.FEAT_STATIC_CAT: fsc\n",
    "                        }\n",
    "                        for (target, start, fsr, fdr) in zip(target[:, :-custom_ds_metadata['prediction_length']],\n",
    "                                                             custom_ds_metadata['start'],\n",
    "                                                             covariate_s,\n",
    "                                                             covariate_d[:, :-custom_ds_metadata['prediction_length']],\n",
    "#                                                              feat_static_cat\n",
    "                                                            )],\n",
    "                      freq=custom_ds_metadata['freq'])\n",
    "\n",
    "test_ds = ListDataset([{FieldName.TARGET: target,\n",
    "                        FieldName.START: start,\n",
    "#                         FieldName.FEAT_STATIC_REAL: fsr,\n",
    "                        FieldName.FEAT_DYNAMIC_REAL: [fdr],\n",
    "#                          FieldName.FEAT_STATIC_CAT: fsc\n",
    "                       }\n",
    "                       for (target, start, fsr, fdr) in zip(target,\n",
    "                                                  custom_ds_metadata['start'],\n",
    "                                                  covariate_s,\n",
    "                                                  covariate_d,\n",
    "#                                                             feat_static_cat\n",
    "                                                      )],\n",
    "                     freq=custom_ds_metadata['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_series = to_pandas(test_entry)\n",
    "# train_series = to_pandas(train_entry)\n",
    "\n",
    "# fig, ax = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 7))\n",
    "\n",
    "# train_series.plot(ax=ax[0])\n",
    "# ax[0].grid(which=\"both\")\n",
    "# ax[0].legend([\"train series\"], loc=\"upper left\")\n",
    "\n",
    "# test_series.plot(ax=ax[1])\n",
    "# ax[1].axvline(train_series.index[-1], color='r') # end of train dataset\n",
    "# ax[1].grid(which=\"both\")\n",
    "# ax[1].legend([\"test series\", \"end of train series\"], loc=\"upper left\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.n_beats import NBEATSEstimator\n",
    "from gluonts.model.transformer import TransformerEstimator\n",
    "from gluonts.model.seq2seq import MQRNNEstimator, MQCNNEstimator\n",
    "from gluonts.mx.distribution import StudentTOutput\n",
    "from gluonts.mx.distribution import GaussianOutput,UniformOutput,NegativeBinomialOutput\n",
    "\n",
    "# estimator = DeepAREstimator(\n",
    "# #     num_layers=1,\n",
    "# #     num_cells=40,\n",
    "# #     distr_output = NegativeBinomialOutput(),\n",
    "# #     alpha=0.5,\n",
    "# #     beta=0.5,\n",
    "#     prediction_length=custom_ds_metadata['prediction_length'],\n",
    "# #     context_length=custom_ds_metadata['context_length'],\n",
    "#     freq=custom_ds_metadata['freq'],\n",
    "# #     use_feat_dynamic_real=True,\n",
    "# #     use_feat_static_real=True,\n",
    "# #     use_feat_static_cat=True,\n",
    "# #     cardinality=cat_cardinality,\n",
    "# #     lags_seq=[1],\n",
    "#     trainer=Trainer(ctx=\"cpu\",#gpu(0),\n",
    "#                     epochs=10,\n",
    "#                     learning_rate=1e-3,\n",
    "#                     num_batches_per_epoch=100,\n",
    "#                     batch_size=32,\n",
    "#                    )\n",
    "# )\n",
    "\n",
    "\n",
    "from model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "estimator = SimpleFeedForwardEstimator(\n",
    "    num_hidden_dimensions=[40],\n",
    "    distr_output = StudentTOutput(),\n",
    "    prediction_length=custom_ds_metadata['prediction_length'],\n",
    "    context_length=custom_ds_metadata['prediction_length'],\n",
    "    freq=custom_ds_metadata['freq'],\n",
    "    trainer=Trainer(ctx=\"cpu\",#gpu(0),\n",
    "                    epochs=10,\n",
    "                    learning_rate=1e-3,\n",
    "                    num_batches_per_epoch=100,\n",
    "                    batch_size=32,\n",
    "                   )\n",
    ")\n",
    "\n",
    "# from model.dnn_prob import MyProbEstimator\n",
    "# from model.dnn_value import MyEstimator\n",
    "# estimator = MyProbEstimator(\n",
    "#     prediction_length=custom_ds_metadata['prediction_length'],\n",
    "#     context_length=custom_ds_metadata['context_length'],\n",
    "#     freq=custom_ds_metadata['freq'],\n",
    "#     distr_output = NegativeBinomialOutput(),\n",
    "#     num_cells=40,\n",
    "#     scaling=True,\n",
    "#     trainer=Trainer(ctx=\"cpu\",#gpu(0),\n",
    "#                     epochs=20,\n",
    "#                     learning_rate=1e-3,\n",
    "#                     num_batches_per_epoch=100,\n",
    "#                     batch_size=32,\n",
    "#                     hybridize=False, \n",
    "#                    )\n",
    "# )\n",
    "\n",
    "# from model.rnn import MyProbRNNEstimator\n",
    "# estimator = MyProbRNNEstimator(\n",
    "#         prediction_length=custom_ds_metadata['prediction_length'],\n",
    "#         context_length=custom_ds_metadata['prediction_length'],\n",
    "#         freq=custom_ds_metadata['freq'],\n",
    "#         num_cells=40,\n",
    "#         num_layers=2,\n",
    "#         distr_output=GaussianOutput(),\n",
    "#         trainer=Trainer(ctx=\"cpu\",\n",
    "#                         epochs=5,\n",
    "#                         learning_rate=1e-3,\n",
    "#                         hybridize=False,\n",
    "#                         num_batches_per_epoch=100\n",
    "#                        )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate from ``lr_scheduler`` has been overwritten by ``learning_rate`` in optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 161.80it/s, epoch=1/10, avg_epoch_loss=0.31]\n",
      "100%|██████████| 100/100 [00:00<00:00, 145.24it/s, epoch=2/10, avg_epoch_loss=-.394]\n",
      "100%|██████████| 100/100 [00:00<00:00, 195.53it/s, epoch=3/10, avg_epoch_loss=-.948]\n",
      "100%|██████████| 100/100 [00:00<00:00, 179.93it/s, epoch=4/10, avg_epoch_loss=-1.37]\n",
      "100%|██████████| 100/100 [00:00<00:00, 194.19it/s, epoch=5/10, avg_epoch_loss=-1.61]\n",
      "100%|██████████| 100/100 [00:00<00:00, 243.96it/s, epoch=6/10, avg_epoch_loss=-1.72]\n",
      "100%|██████████| 100/100 [00:00<00:00, 237.64it/s, epoch=7/10, avg_epoch_loss=-1.92]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "predictor = estimator.train(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.serialize(Path(\"/tmp/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_ds,  # test dataset\n",
    "    predictor=predictor,  # predictor\n",
    "    num_samples=100,  # number of sample paths we want for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_index = 3\n",
    "ts_entry = tss[country_index]\n",
    "forecast_entry = forecasts[country_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_prob_forecasts(ts_entry, forecast_entry):\n",
    "    plot_length = 150\n",
    "    prediction_intervals = (50.0, 90.0)\n",
    "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series\n",
    "    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.legend(legend, loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "plot_prob_forecasts(ts_entry, forecast_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ts_entry[:5]).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_entry = next(iter(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of sample paths: {forecast_entry.num_samples}\")\n",
    "print(f\"Dimension of samples: {forecast_entry.samples.shape}\")\n",
    "print(f\"Start date of the forecast window: {forecast_entry.start_date}\")\n",
    "print(f\"Frequency of the time series: {forecast_entry.freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean of the future window:\\n {forecast_entry.mean}\")\n",
    "print(f\"0.5-quantile (median) of the future window:\\n {forecast_entry.quantile(0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(agg_metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metrics.plot(x='MSIS', y='MASE', kind='scatter')\n",
    "plt.grid(which=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
